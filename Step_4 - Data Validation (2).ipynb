{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0410c861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importants\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import ElasticNetCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd337c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation Starts Here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40ecd93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pickled_X5', 'rb') as f:\n",
    "    X = pickle.load(f)\n",
    "\n",
    "with open('pickled_Y5', 'rb') as f:\n",
    "    Y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1be4ddc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training and testing sets from the best model\n",
    "# Test size is 20% or total dataframe\n",
    "# Specifying a random_state means we will get the same shuffle each time\n",
    "# Setting a random_state is helpful for debugging since it ensures the model will train on the same data each time this is run\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=.2, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d07d34c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the type of regression that will be used in cross-validation\n",
    "# This variable will be referenced later\n",
    "\n",
    "L_Reg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f938452b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, # number of folds\n",
    "     shuffle=True, # randomizes order of rows before split\n",
    "     random_state = 7) # using same number in future code/ rerunning allows us to replicate the split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bf0ccbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to check for overfitting by comparing training scores to val scores\n",
    "val_and_train_scores = cross_validate(L_Reg, X_train, y_train, \n",
    "                                      cv=kf, \n",
    "                                      scoring='r2',\n",
    "                                      return_train_score=True)\n",
    "# print('Validation Score:  ', val_and_train_scores['test_score']) # test_score is validation score\n",
    "# print('Train Score: ', val_and_train_scores['train_score']) # train_score is training score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2ae23f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "computer_model = sm.OLS(Y, X)\n",
    "results = computer_model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ab3a649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Score:... 0.776 +- 0.027\n",
      "Train Score:........ 0.787 +- 0.007\n",
      "Test Score:......... 0.703\n",
      "Overfit Score:...... 0.0114\n"
     ]
    }
   ],
   "source": [
    "# Printing the mean validation score with standard deviation, rounded to 2 decimals\n",
    "val_score = np.round(np.mean(val_and_train_scores['test_score']), 3)\n",
    "std_val = np.round(np.std(val_and_train_scores['test_score']), 3)\n",
    "\n",
    "# Printing the mean training score with standard deviation, rounded to 2 decimals\n",
    "train_score = np.round(np.mean(val_and_train_scores['train_score']), 3)\n",
    "std_train = np.round(np.std(val_and_train_scores['train_score']), 3)\n",
    "\n",
    "# Resulting Test Score\n",
    "y_pred = results.predict(X_test)\n",
    "test_r2 = r2_score(y_test, y_pred)\n",
    "test_score = np.round(test_r2, 3)\n",
    "\n",
    "# Print the difference between the training score and testing score to ensure the model isn't overfitting\n",
    "overfit = np.round(abs(np.mean(val_and_train_scores['test_score'])-np.mean(val_and_train_scores['train_score'])), 4)\n",
    "\n",
    "\n",
    "print('Validation Score:...', val_score, \"+-\", std_val)\n",
    "print('Train Score:........', train_score, \"+-\", std_train)\n",
    "print('Test Score:.........', test_score)\n",
    "print('Overfit Score:......', overfit)\n",
    "# Since there is only a very small difference between validation and training scores, we can infer the model is not overfitting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fae9152f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elastic Net Regularization Starts Here (Cross-Validation of this model at the end):\n",
    "lambdas = [0.0001, 0.01, 0.1, 0.3, 0.5, 0.7, 1]\n",
    "L1s = [0.1, 0.5, 0.7, 0.9, 0.95, 0.99, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54a57817",
   "metadata": {},
   "outputs": [],
   "source": [
    "enet_model = ElasticNetCV(alphas=lambdas,\n",
    "                         l1_ratio = L1s,\n",
    "                         cv=kf, \n",
    "                          random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1a51039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElasticNetCV(alphas=[0.0001, 0.01, 0.1, 0.3, 0.5, 0.7, 1],\n",
       "             cv=KFold(n_splits=5, random_state=7, shuffle=True),\n",
       "             l1_ratio=[0.1, 0.5, 0.7, 0.9, 0.95, 0.99, 1], random_state=7)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training on 80% of available data, as separated earlier\n",
    "enet_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4449dff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now to cross-validate our elastic net regularization:\n",
    "# Need to check for overfitting by comparing training scores to val scores\n",
    "enet_val_and_train_scores = cross_validate(enet_model, X_train, y_train, \n",
    "                                      cv=kf, \n",
    "                                      scoring='r2',\n",
    "                                      return_train_score=True)\n",
    "# print('Validation Score:  ', enet_val_and_train_scores['test_score']) # test_score is validation score\n",
    "# print('Train Score: ', enet_val_and_train_scores['train_score']) # train_score is training score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc8ff4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E-Net Validation Score:... 0.78 +- 0.029\n",
      "E-Net Train Score:........ 0.785 +- 0.007\n",
      "E-Net Test Score:......... 0.694\n",
      "Overfit Score:............ 0.0058\n"
     ]
    }
   ],
   "source": [
    "# Printing the mean validation score with standard deviation, rounded to 2 decimals\n",
    "enet_val_score = np.round(np.mean(enet_val_and_train_scores['test_score']), 3)\n",
    "enet_std_val = np.round(np.std(enet_val_and_train_scores['test_score']), 3)\n",
    "\n",
    "# Printing the mean training score with standard deviation, rounded to 2 decimals\n",
    "enet_train_score = np.round(np.mean(enet_val_and_train_scores['train_score']), 3)\n",
    "enet_std_train = np.round(np.std(enet_val_and_train_scores['train_score']), 3)\n",
    "\n",
    "# Final score on testing data (Data that was not included in training the model)\n",
    "# Test Score from Elastic Net with 5-k-folds\n",
    "enet_test_score = np.round(enet_model.score(X_test, y_test), 3) \n",
    "\n",
    "# Print the difference between the training score and testing score to ensure the model isn't overfitting\n",
    "enet_overfit = np.round(abs(np.mean(enet_val_and_train_scores['test_score'])-np.mean(enet_val_and_train_scores['train_score'])), 4)\n",
    "\n",
    "print('E-Net Validation Score:...', enet_val_score, \"+-\", enet_std_val)\n",
    "print('E-Net Train Score:........', enet_train_score, \"+-\", enet_std_train)\n",
    "print('E-Net Test Score:.........', enet_test_score)\n",
    "print('Overfit Score:............', enet_overfit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d91dd1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conclusions:\n",
    "\n",
    "# It can be seen that even though both the price_log model and enet model get the same average score for training\n",
    "# and validation, the scores on each k-fold are not exactly the same. This is a sanity check to ensure we are in fact testing\n",
    "# different models.\n",
    "\n",
    "# It can be concluded that the enet model does not significantly improve the R^2 score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
